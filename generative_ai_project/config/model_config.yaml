# ==============================================================================
# MODEL CONFIGURATION - LLM Ransomware Profiling Project
# ==============================================================================

# ------------------------------------------------------------------------------
# CLUSTER CONFIGURATION
# ------------------------------------------------------------------------------
unibs_cluster:
  base_url: "https://gpustack.ing.unibs.it/v1"
  # API Key loaded from environment variable GPUSTACK_API_KEY
  # DO NOT insert the key here! Use .env file
  
  # Notes from handbook:
  # - Experimental service, no availability guarantees
  # - Accessible only from UniBS network or via VPN
  # - Do not send sensitive/personal data


# ------------------------------------------------------------------------------
# AVAILABLE MODELS (from Handbook Section 5)
# ------------------------------------------------------------------------------
available_models:
  llm:
    - qwen3              # Hugging Face/unsloth/Qwen3-4B-GGUF
    - phi4-mini          # Hugging Face/unsloth/Phi-4-mini-instruct-GGUF
    - phi4               # Hugging Face/bartowski/phi-4-GGUF
    - llama3.2           # Hugging Face/bartowski/Llama-3.2-3B-Instruct-GGUF
    - gpt-oss            # Hugging Face/unsloth/gpt-oss-20b-GGUF
    - granite3.3         # Hugging Face/ibm-granite/granite-3.3-2b-instruct-GGUF
    - gemma3             # Hugging Face/bartowski/google_gemma-3-1b-it-GGUF
  
  embedding:
    - qwen3-embedding           # Hugging Face/Qwen/Qwen3-Embedding-4B-GGUF
    - nomic-embed-text-v1.5     # Hugging Face/nomic-ai/nomic-embed-text-v1.5-GGUF


# ------------------------------------------------------------------------------
# ACTIVE MODEL (for single execution)
# ------------------------------------------------------------------------------
active_model: "phi4-mini"


# ------------------------------------------------------------------------------
# ENSEMBLE MODELS (for multi-model consensus)
# ------------------------------------------------------------------------------
# List of models to run in parallel for consensus voting
ensemble_models:
  - phi4-mini
  - qwen3
  - llama3.2

# Note: If ensemble_models is empty or has only 1 element,
# consensus will not be executed


# ------------------------------------------------------------------------------
# STANDARD LLM PARAMETERS (Handbook Section 6)
# ------------------------------------------------------------------------------
# These are default parameters applied to ALL models
# unless overridden in model_specific_params
llm_parameters:
  temperature: 0.6              # Determinism (0.0) vs Creativity (1.0)
  top_p: 0.95                   # Nucleus sampling
  max_tokens: 1024              # Maximum response length
  frequency_penalty: 0          # Repetition penalty (0.0 - 2.0)
  presence_penalty: 0           # Topic coverage penalty (0.0 - 2.0)


# ------------------------------------------------------------------------------
# MODEL-SPECIFIC PARAMETERS
# ------------------------------------------------------------------------------
# Override parameters for specific models (based on best practices)
model_specific_params:
  
  # GPT-OSS: Has larger context, use higher max_tokens
  gpt-oss:
    max_tokens: 4096
    temperature: 0.8
  
  # Phi4-mini: Good for creativity, higher temperature
  phi4-mini:
    temperature: 0.8
  
  # Qwen3: Excellent for precision analysis, lower temperature
  qwen3:
    temperature: 0.6
  
  # Llama3.2: Balanced
  llama3.2:
    temperature: 0.7
  
  # Phi4: Advanced model, more deterministic for complex tasks
  phi4:
    temperature: 0.7
    max_tokens: 2048
  
  # Granite3.3: Concise responses, moderate temperature
  granite3.3:
    temperature: 0.7
  
  # Gemma3: Good for simple explanations
  gemma3:
    temperature: 0.8


# ------------------------------------------------------------------------------
# EMBEDDING CONFIGURATION (Handbook Section 7)
# ------------------------------------------------------------------------------
embedding:
  default_model: "nomic-embed-text-v1.5"
  
  # Parameters for future RAG/semantic search features
  vector_dimensions:
    qwen3-embedding: 4096
    nomic-embed-text-v1.5: 768


# ------------------------------------------------------------------------------
# PROCESSING & RETRY CONFIGURATION
# ------------------------------------------------------------------------------
processing:
  max_workers: 4                # Parallel threads for chat processing
  chunk_max_chars: 10000        # Max prompt length before chunking
  
  # Retry logic
  retry:
    max_attempts: 3             # Max retries for failed calls
    backoff_factor: 2           # Multiplier for exponential backoff
    retry_on_errors:
      - "APITimeoutError"
      - "RateLimitError"


# ------------------------------------------------------------------------------
# PATHS (relative to project root)
# ------------------------------------------------------------------------------
paths:
  raw_data: "data/raw/messages.json"
  outputs: "data/outputs"
  logs: "logs"
  config: "config"
  prompts: "config/prompt_templates.yaml"
  few_shot: "config/few_shot_examples"  # NOTE: not "few_shot_examples1"


# ------------------------------------------------------------------------------
# CONSENSUS CONFIGURATION
# ------------------------------------------------------------------------------
consensus:
  enabled: true                 # Enable consensus only if ensemble_models > 1
  strategy: "majority_vote"     # Strategy: majority_vote | weighted_average
  min_agreement: 0.6            # Minimum agreement threshold (60%)
  
  # Weights for weighted_average (optional)
  model_weights:
    phi4-mini: 1.0
    qwen3: 1.2                  # Higher weight for precision
    llama3.2: 1.0


# ------------------------------------------------------------------------------
# LOGGING & DEBUG
# ------------------------------------------------------------------------------
logging:
  level: "INFO"                 # DEBUG | INFO | WARNING | ERROR
  console_level: "WARNING"      # Only warning+ on console (clean stdout)
  file_level: "DEBUG"           # Everything to file for debugging
  
  format:
    console: "⚠️  %(message)s"
    file: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  
  # Save reasoning_content 
  save_reasoning: true          # Save thinking/reasoning field if available
  reasoning_suffix: "_reasoning.txt"


# ------------------------------------------------------------------------------
# FEATURE FLAGS
# ------------------------------------------------------------------------------
features:
  use_few_shot: true            # Enable few-shot learning
  validate_json_output: true    # Strict JSON validation before saving
  parallel_model_execution: false  # Parallelize models (experimental)
  save_intermediate_outputs: true  # Save intermediate outputs for debug


# ------------------------------------------------------------------------------
# DATASET CONFIGURATION
# ------------------------------------------------------------------------------
dataset:
  source: "local"               # local | github
  github_repo: "Casualtek/Ransomchats"
  max_chats_per_group: null     # null = all, otherwise limit
  ignored_groups:
    - ".git"
    - ".github"
    - "parsers"
    - "src"


# ------------------------------------------------------------------------------
# EXPERIMENTAL (Do not use in production)
# ------------------------------------------------------------------------------
experimental:
  enable_streaming: false       # Streaming responses (not supported by cluster)
  cache_responses: false        # Local cache for LLM responses
  use_reasoning_in_prompt: false  # Include previous reasoning in prompt
