# ==============================================================================
# MODEL CONFIGURATION - LLM Ransomware Profiling Project
# ==============================================================================


# ------------------------------------------------------------------------------
# API CONFIGURATION
# ------------------------------------------------------------------------------
api_service:
  base_url: "https://gpustack.ing.unibs.it/v1"
  # API Key loaded from environment variable GPUSTACK_API_KEY
  # API key should be stored in environment variables, not in this file


# ------------------------------------------------------------------------------
# AVAILABLE MODELS
# ------------------------------------------------------------------------------
available_models:
  llm:
    - qwen3              # Hugging Face/unsloth/Qwen3-4B-GGUF
    - phi4-mini          # Hugging Face/unsloth/Phi-4-mini-instruct-GGUF
    - phi4               # Hugging Face/bartowski/phi-4-GGUF
    - llama3.2           # Hugging Face/bartowski/Llama-3.2-3B-Instruct-GGUF
    - gpt-oss            # Hugging Face/unsloth/gpt-oss-20b-GGUF
    - granite3.3         # Hugging Face/ibm-granite/granite-3.3-2b-instruct-GGUF
    - gemma3             # Hugging Face/bartowski/google_gemma-3-1b-it-GGUF
  
  embedding:
    - qwen3-embedding           # Hugging Face/Qwen/Qwen3-Embedding-4B-GGUF
    - nomic-embed-text-v1.5     # Hugging Face/nomic-ai/nomic-embed-text-v1.5-GGUF


# ------------------------------------------------------------------------------
# ACTIVE MODEL
# ------------------------------------------------------------------------------
active_model: "qwen3"


# ------------------------------------------------------------------------------
# ENSEMBLE MODELS
# ------------------------------------------------------------------------------
# Models to run in parallel for consensus-based analysis
ensemble_models:
  - phi4
  - gpt-oss
  - phi4-mini


# ------------------------------------------------------------------------------
# STANDARD LLM PARAMETERS
# ------------------------------------------------------------------------------
# Default parameters applied to all models unless overridden
llm_parameters:
  temperature: 0.8              # Determinism (0.0) vs Creativity (1.0)
  top_p: 0.95                   # Nucleus sampling threshold
  max_tokens: 4096              # Maximum response length
  frequency_penalty: 0          # Repetition penalty (0.0 - 2.0)
  presence_penalty: 0           # Topic coverage penalty (0.0 - 2.0)


# ------------------------------------------------------------------------------
# MODEL-SPECIFIC PARAMETERS
# ------------------------------------------------------------------------------
# Parameter overrides for individual models
model_specific_params:
  
  gpt-oss:
    max_tokens: 4096
    temperature: 0.8
  
  phi4-mini:
    temperature: 0.8
  
  qwen3:
    temperature: 0.6
  
  llama3.2:
    temperature: 0.7
  
  phi4:
    temperature: 0.7
    max_tokens: 2048
  
  granite3.3:
    temperature: 0.7
  
  gemma3:
    temperature: 0.8


# ------------------------------------------------------------------------------
# EMBEDDING CONFIGURATION
# ------------------------------------------------------------------------------
embedding:
  default_model: "nomic-embed-text-v1.5"
  
  # Vector dimensions for embedding models
  vector_dimensions:
    qwen3-embedding: 4096
    nomic-embed-text-v1.5: 768


# ------------------------------------------------------------------------------
# PROCESSING & RETRY CONFIGURATION
# ------------------------------------------------------------------------------
processing:
  max_workers: 3                # Parallel threads for chat processing
  chunk_max_chars: 10000        # Maximum prompt length before chunking
  
  # Retry mechanism configuration
  retry:
    max_attempts: 3             # Maximum retry attempts for failed API calls
    backoff_factor: 2           # Exponential backoff multiplier
    retry_on_errors:
      - "APITimeoutError"
      - "RateLimitError"


# ------------------------------------------------------------------------------
# PATHS
# ------------------------------------------------------------------------------
paths:
  raw_data: "data/raw/messages.json"
  outputs: "data/outputs"
  logs: "logs"
  config: "config"
  prompts: "config/prompt_templates.yaml"
  few_shot: "config/few_shot_examples"


# ------------------------------------------------------------------------------
# CONSENSUS CONFIGURATION
# ------------------------------------------------------------------------------
consensus:
  enabled: true                 # Enable consensus mode with multiple models
  strategy: "majority_vote"     # Options: majority_vote | weighted_average
  min_agreement: 0.6            # Minimum agreement threshold (60%)
  
  # Model weights for weighted_average strategy
  model_weights:
    phi4-mini: 1.0
    qwen3: 1.2
    llama3.2: 1.0


# ------------------------------------------------------------------------------
# LOGGING & DEBUG
# ------------------------------------------------------------------------------
logging:
  level: "INFO"                 # DEBUG | INFO | WARNING | ERROR
  console_level: "WARNING"      # Console output level
  file_level: "DEBUG"           # File logging level
  
  format:
    console: "%(message)s"
    file: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  
  # Reasoning output configuration
  save_reasoning: true          # Save model reasoning output if available
  reasoning_suffix: "_reasoning.txt"


# ------------------------------------------------------------------------------
# FEATURE FLAGS
# ------------------------------------------------------------------------------
features:
  use_few_shot: true            # Enable few-shot learning
  validate_json_output: true    # Validate JSON output before saving
  parallel_model_execution: false  # Execute models in parallel
  save_intermediate_outputs: true  # Save intermediate processing outputs


# ------------------------------------------------------------------------------
# DATASET CONFIGURATION
# ------------------------------------------------------------------------------
dataset:
  source: "local"               # Source type: local | github
  github_repo: "Casualtek/Ransomchats"
  max_chats_per_group: null     # Maximum chats per group (null for unlimited)
  ignored_groups:
    - ".git"
    - ".github"
    - "parsers"
    - "src"


# ------------------------------------------------------------------------------
# EXPERIMENTAL FEATURES
# ------------------------------------------------------------------------------
experimental:
  enable_streaming: false         # Enable streaming responses
  cache_responses: false          # Enable local response caching
  use_reasoning_in_prompt: false  # Include previous reasoning in prompts
